{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Historic Global Temperatures \n",
    "### Data Engineering Capstone Project\n",
    "By Kyle McMillan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Project Summary <a name=\"summary\"></a>\n",
    "The aim of this project to create and data lake and ETL pipeline to store data related to historic city temperatures and atmospheric CO<sup>2</sup> PPM values. This data can be used to investigate global temperature increases.   \n",
    "\n",
    "The project follows the follow steps:  \n",
    "[Step 1: Scope the Project and Gathering Data](#step1)  \n",
    "[Step 2: Exploring and Assessing the Data](#step2)  \n",
    "[Step 3: Defining the Data Model](#step3)  \n",
    "[Step 4: Running the Pipelines to Model the Data](#step4)  \n",
    "[Step 5: Project Write Up](#step5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DateType, FloatType\n",
    "from pyspark.sql.functions import udf, year, month, dayofmonth, hour, weekofyear, date_format, col, to_date\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Set the current date\n",
    "today = datetime.now().date().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Raw data input filepaths.\n",
    "temp_file_name = 'temp_data/GlobalLandTemperaturesByCity.csv'\n",
    "co2_file_name = 'co2_data/co2-ppm-daily_json.json'\n",
    "\n",
    "#Statging parquet output filepaths.\n",
    "temp_sparkdf_path = f\"parquet_files/{today}-city_temp_staging.parquet\"\n",
    "co2_sparkdf_path = f\"parquet_files/{today}-global_co2_staging.parquet\"\n",
    "\n",
    "#Final table parquet output filepaths.\n",
    "fact_temp_table_path = f\"parquet_files/{today}-fact_temp_table.parquet\"\n",
    "dim_city_table_path = f\"parquet_files/{today}-dim_city_table.parquet\"\n",
    "dim_co2_table_path = f\"parquet_files/{today}-dim_co2_table.parquet\"\n",
    "dim_date_table_path = f\"parquet_files/{today}-dim_date_table.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gathering Data <a name=\"step1\"></a>\n",
    "\n",
    "#### Scope \n",
    "The scope of this project is to create an ETL pipeline for processing, cleaning and storing data. The data is global country and city historic temperatures, and global historic CO<sup>2</sup> PPM values.\n",
    "\n",
    "The output of the ETL pipeline is the processed data stored in snowflake schema model saved to parquet files on the local system. \n",
    "\n",
    "Tools used: \n",
    "- python \n",
    "- pandas \n",
    "- pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "This project contains datasets from two different sources. \n",
    "\n",
    "##### Earth temperature data\n",
    "Source: [Kaggle dataset](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data#GlobalLandTemperaturesByCity.csv)  \n",
    "filename: GlobalLandTemperaturesByCity.csv  \n",
    "Type: CSV  \n",
    "Size: 508.15 MB  \n",
    "Shape: 8599212, 7  \n",
    "Description: This data contains historic recorded temperatures from cities around the world on a daily basis. The dates start from the 18<sup>th</sup> century but not every city starts back to that point. Also, the easliest recordings are not on a daily basis. Very early dates are sporadic.   \n",
    "Visualisation of the temperature data:  \n",
    "![Temp visuals](img/global_temp.PNG)\n",
    "\n",
    "\n",
    "##### Earth CO<sup>2</sup> levels data\n",
    "Source: [CO2 PPM - Trends in Atmospheric Carbon Dioxide](https://datahub.io/core/co2-ppm-daily)  \n",
    "filename: co2-ppm-daily_json.json  \n",
    "Type: JSON  \n",
    "Size: 1 MB  \n",
    "Shape: 19413, 2  \n",
    "Description: This data contains historic recorded CO<sup>2</sup> PPM levels recorded from a monitoring station located in Hawaii. The dates go back as far as 1958/3/30. The earlist dates are not on a daily basis, but most days have been recorded.  \n",
    "Visualisation of the CO<sup>2</sup> PPM level data:  \n",
    "![CO2 visuals](img/global_co2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Historic global temperature data\n",
    "Data viewed as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data from CSV file\n",
    "temp_df = pd.read_csv(temp_file_name)\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Global CO<sup>2</sup> PPM levels\n",
    "Data viewed as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1958-03-30</td>\n",
       "      <td>316.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1958-03-31</td>\n",
       "      <td>316.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1958-04-02</td>\n",
       "      <td>317.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1958-04-03</td>\n",
       "      <td>317.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1958-04-04</td>\n",
       "      <td>317.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   value\n",
       "0 1958-03-30  316.16\n",
       "1 1958-03-31  316.40\n",
       "2 1958-04-02  317.67\n",
       "3 1958-04-03  317.76\n",
       "4 1958-04-04  317.09"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data from JSON\n",
    "co2_df = pd.read_json(co2_file_name)\n",
    "co2_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load the data into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load and display the temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Set the schema for the temperature data.\n",
    "temp_schema = StructType([StructField(\"datestamp\", DateType(), False),\n",
    "                            StructField(\"avg_temp\", FloatType(), True),\n",
    "                            StructField(\"avg_temp_uncert\", FloatType(), True),\n",
    "                            StructField(\"city\", StringType(), False),\n",
    "                            StructField(\"country\", StringType(), False),\n",
    "                            StructField(\"latitude\", StringType(), False),\n",
    "                            StructField(\"longitude\", StringType(), False)])\n",
    "\n",
    "#Load the CSV into spark with the schema.\n",
    "temp_df_spark = spark.read.csv(temp_file_name, temp_schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datestamp: date (nullable = true)\n",
      " |-- avg_temp: float (nullable = true)\n",
      " |-- avg_temp_uncert: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema.\n",
    "temp_df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+-----+-------+--------+---------+\n",
      "|datestamp |avg_temp|avg_temp_uncert|city |country|latitude|longitude|\n",
      "+----------+--------+---------------+-----+-------+--------+---------+\n",
      "|1743-11-01|6.068   |1.737          |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1743-12-01|null    |null           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-01-01|null    |null           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-02-01|null    |null           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-03-01|null    |null           |Århus|Denmark|57.05N  |10.33E   |\n",
      "+----------+--------+---------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the first 5 rows.\n",
    "temp_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load and display the temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# co2_schema = StructType([StructField(\"datestamp\", StringType(), False), \n",
    "#                          StructField(\"co2_ppm\", StringType(), False),])\n",
    "\n",
    "#Load the JSON file into spark.\n",
    "co2_df_spark =spark.read.json(co2_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema.\n",
    "co2_df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|date      |value |\n",
      "+----------+------+\n",
      "|1958-03-30|316.16|\n",
      "|1958-03-31|316.40|\n",
      "|1958-04-02|317.67|\n",
      "|1958-04-03|317.76|\n",
      "|1958-04-04|317.09|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the first 5 rows.\n",
    "co2_df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Write parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath: parquet_files/2020-02-26-city_temp_staging.parquet\n"
     ]
    }
   ],
   "source": [
    "# Write city temperature data to parquet file.\n",
    "temp_df_spark.write.parquet(temp_sparkdf_path, mode=\"overwrite\")\n",
    "print(f\"filepath: {temp_sparkdf_path}\")\n",
    "\n",
    "# Read the parquet file back into a Spark dataframe.\n",
    "temp_df_spark = spark.read.parquet(temp_sparkdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath: parquet_files/2020-02-26-global_co2_staging.parquet\n"
     ]
    }
   ],
   "source": [
    "# Write global co2 data to parquet file.\n",
    "co2_df_spark.write.parquet(co2_sparkdf_path, mode=\"overwrite\")\n",
    "print(f\"filepath: {co2_sparkdf_path}\")\n",
    "\n",
    "# Read the parquet file back into a Spark dataframe.\n",
    "co2_df_spark = spark.read.parquet(co2_sparkdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Exploring and Assessing the Data <a name=\"step2\"></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Exploring the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Quality issues\n",
    "* Staging global temperature data: \n",
    "    * Some temperature values have null.\n",
    "  \n",
    "* Staging CO<sup>2</sup> PPM data:\n",
    "    * No quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Steps to clean the data.\n",
    "* Staging global temperature data:\n",
    "    * null values to be dropped.\n",
    "\n",
    "* Staging CO<sup>2</sup> PPM data:\n",
    "    * No quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove null values from the global temperature data.\n",
    "temp_df_spark_clean = temp_df_spark.where(col(\"avg_temp\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Defining the Data Model <a name=\"step3\"></a>\n",
    "#### 3.1 Conceptual Data Model\n",
    "The model that was chosen for this project is a snowflake model. This was chosen due to the nature of the data, where the main temperature values are listed by date and city where as the CO<sup>2</sup> values are listed only by date.   \n",
    "The model consists of the following:  \n",
    "- Fact table:\n",
    "    - fact_temp_table\n",
    "- Dimension tables:\n",
    "    - dim_city_table\n",
    "    - dim_date_table\n",
    "    - dim_co2_table\n",
    "\n",
    "Schema visulisation  \n",
    "![Snowflake schema](img/tables.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "1. ETL script gathers the input data; Temperature CSV and CO2 JSON files.\n",
    "1. The raw sourced data is read into Spark dataframes and stored locally as parquet staging files.\n",
    "1. Each staging parquet file is read back into Spark dataframes and cleaned as necessay.\n",
    "1. Data is extracted and used to create the output tables.\n",
    "1. Quality checks are performed\n",
    "1. The final tables are written as parquet files to the local filesystem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Running the Pipelines to Model the Data <a name=\"step4\"></a>\n",
    "#### 4.1 Create the data model\n",
    "Building the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Extract columns to create the temperature table\n",
    "fact_temp_table = temp_df_spark_clean.select('datestamp', 'avg_temp', 'avg_temp_uncert','city', 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datestamp: date (nullable = true)\n",
      " |-- avg_temp: float (nullable = true)\n",
      " |-- avg_temp_uncert: float (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema.\n",
    "fact_temp_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+--------+-------+\n",
      "|datestamp |avg_temp|avg_temp_uncert|city    |country|\n",
      "+----------+--------+---------------+--------+-------+\n",
      "|1907-07-01|14.739  |0.624          |Edmonton|Canada |\n",
      "|1907-08-01|12.001  |0.603          |Edmonton|Canada |\n",
      "|1907-09-01|8.319   |0.324          |Edmonton|Canada |\n",
      "|1907-10-01|6.339   |0.77           |Edmonton|Canada |\n",
      "|1907-11-01|-1.668  |0.787          |Edmonton|Canada |\n",
      "+----------+--------+---------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the first 5 rows.\n",
    "fact_temp_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Extract columns to create the city table and only keep one row for each city.\n",
    "dim_city_table = temp_df_spark_clean.select('city', 'country', 'longitude','latitude').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema.\n",
    "dim_city_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+---------+--------+\n",
      "|city        |country|longitude|latitude|\n",
      "+------------+-------+---------+--------+\n",
      "|Erzincan    |Turkey |39.54E   |39.38N  |\n",
      "|Khorramshahr|Iran   |48.00E   |29.74N  |\n",
      "|Korla       |China  |85.21E   |40.99N  |\n",
      "|Lasa        |China  |90.46E   |29.74N  |\n",
      "|Lille       |France |3.80E    |50.63N  |\n",
      "+------------+-------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the first 5 rows.\n",
    "dim_city_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Extract columns to create the co2 table\n",
    "dim_co2_table = co2_df_spark.withColumn('datestamp', to_date(col(\"date\"),'yyyy-MM-dd')) \\\n",
    "            .select('datestamp', co2_df_spark.value.cast('float').alias('co2_ppm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datestamp: date (nullable = true)\n",
      " |-- co2_ppm: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema.\n",
    "dim_co2_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|datestamp |co2_ppm|\n",
      "+----------+-------+\n",
      "|1958-03-30|316.16 |\n",
      "|1958-03-31|316.4  |\n",
      "|1958-04-02|317.67 |\n",
      "|1958-04-03|317.76 |\n",
      "|1958-04-04|317.09 |\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the first 5 rows.\n",
    "dim_co2_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create date based information from the original datestamp column.\n",
    "get_weekday = udf(lambda x: datetime.strptime(str(x), '%Y-%m-%d').weekday())\n",
    "\n",
    "#Extract columns to create the date table\n",
    "dim_date_table = temp_df_spark_clean.select('datestamp',\n",
    "                                           year(temp_df_spark_clean.datestamp).alias('year'), \n",
    "                                           month(temp_df_spark_clean.datestamp).alias('month'), \n",
    "                                           dayofmonth(temp_df_spark_clean.datestamp).alias('day'),  \n",
    "                                           weekofyear(temp_df_spark_clean.datestamp).alias('week')) \\\n",
    "                                           .withColumn('weekday', get_weekday(temp_df_spark_clean.datestamp).cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datestamp: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the schema.\n",
    "dim_date_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+----+-------+\n",
      "|datestamp |year|month|day|week|weekday|\n",
      "+----------+----+-----+---+----+-------+\n",
      "|1907-07-01|1907|7    |1  |27  |0      |\n",
      "|1907-08-01|1907|8    |1  |31  |3      |\n",
      "|1907-09-01|1907|9    |1  |35  |6      |\n",
      "|1907-10-01|1907|10   |1  |40  |1      |\n",
      "|1907-11-01|1907|11   |1  |44  |4      |\n",
      "+----------+----+-----+---+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the first 5 rows.\n",
    "dim_date_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Writing the four parquet table files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to: parquet_files/2020-02-26-fact_temp_table.parquet\n",
      "Data written to: parquet_files/2020-02-26-dim_city_table.parquet\n",
      "Data written to: parquet_files/2020-02-26-dim_co2_table.parquet\n",
      "Data written to: parquet_files/2020-02-26-dim_date_table.parquet\n"
     ]
    }
   ],
   "source": [
    "fact_temp_table.write.parquet(fact_temp_table_path, partitionBy=['country', 'city'], mode=\"overwrite\")\n",
    "print(f\"Data written to: {fact_temp_table_path}\")\n",
    "dim_city_table.write.parquet(dim_city_table_path, partitionBy=['country', 'city'], mode=\"overwrite\")\n",
    "print(f\"Data written to: {dim_city_table_path}\")\n",
    "dim_co2_table.write.parquet(dim_co2_table_path, mode=\"overwrite\")\n",
    "print(f\"Data written to: {dim_co2_table_path}\")\n",
    "dim_date_table.write.parquet(dim_date_table_path, mode=\"overwrite\")\n",
    "print(f\"Data written to: {dim_date_table_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Quality checks for this project are:\n",
    "- Insure that all tables have data in them.\n",
    "- Check that important values within the tables have no null values.\n",
    "- Confirm the data types are correct for each tables' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in fact_temp_table: 8235082\n",
      "Number of rows in dim_city_table: 3510\n",
      "Number of rows in dim_co2_table: 19414\n",
      "Number of rows in dim_date_table: 8235082\n"
     ]
    }
   ],
   "source": [
    "#Check to see if the tables are empty.\n",
    "print (f\"Number of rows in fact_temp_table: {fact_temp_table.count()}\")\n",
    "print (f\"Number of rows in dim_city_table: {dim_city_table.count()}\")\n",
    "print (f\"Number of rows in dim_co2_table: {dim_co2_table.count()}\")\n",
    "print (f\"Number of rows in dim_date_table: {dim_date_table.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in fact_temp_table where avg_temp column has null values: 0\n",
      "Number of rows in dim_city_table where city column has null values: 0\n",
      "Number of rows in dim_city_table where country column has null values: 0\n",
      "Number of rows in dim_co2_table where co2_ppm column has null values: 0\n",
      "Number of rows in dim_date_table where datestamp column has null values: 0\n"
     ]
    }
   ],
   "source": [
    "#Check for null values in certian columns in the tables\n",
    "print (f\"Number of rows in fact_temp_table where avg_temp column has null values: {fact_temp_table.where(col('avg_temp').isNull()).count()}\")\n",
    "print (f\"Number of rows in dim_city_table where city column has null values: {dim_city_table.where(col('city').isNull()).count()}\")\n",
    "print (f\"Number of rows in dim_city_table where country column has null values: {dim_city_table.where(col('country').isNull()).count()}\")\n",
    "print (f\"Number of rows in dim_co2_table where co2_ppm column has null values: {dim_co2_table.where(col('co2_ppm').isNull()).count()}\")\n",
    "print (f\"Number of rows in dim_date_table where datestamp column has null values: {dim_date_table.where(col('datestamp').isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types in the fact_temp_table: [('datestamp', 'date'), ('avg_temp', 'float'), ('avg_temp_uncert', 'float'), ('city', 'string'), ('country', 'string')]\n",
      "Data types in the dim_city_table: [('city', 'string'), ('country', 'string'), ('longitude', 'string'), ('latitude', 'string')]\n",
      "Data types in the dim_co2_table: [('datestamp', 'date'), ('co2_ppm', 'float')]\n",
      "Data types in the dim_date_table: [('datestamp', 'date'), ('year', 'int'), ('month', 'int'), ('day', 'int'), ('week', 'int'), ('weekday', 'int')]\n"
     ]
    }
   ],
   "source": [
    "#Check the data types of each table\n",
    "print (f\"Data types in the fact_temp_table: {fact_temp_table.dtypes}\")\n",
    "print (f\"Data types in the dim_city_table: {dim_city_table.dtypes}\")\n",
    "print (f\"Data types in the dim_co2_table: {dim_co2_table.dtypes}\")\n",
    "print (f\"Data types in the dim_date_table: {dim_date_table.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Located in a seperate file \"Data Dictionary.md\" with this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Project Write Up <a name=\"step5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Choice of tools:\n",
    "Python and Spark were chosen for this project, because they are both tools that can scale easily if needed.  \n",
    "\n",
    "As the data used for this project is not very large, all processing was done on a local machine without out connecting to a cluster service such as AWS. Though it would be easy to adjust the original Spark setup to run via a cluster if needed or if this project was to scale up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Data update frequency\n",
    "As this processes daily temperature data, the best time for updating would be at the end of the day UTC+12. This would allow all countries to gather the data for that day in question before being pushed to a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Other scenarios\n",
    "##### - Data increased by 100x\n",
    "If the data for this project was increased 100x then I would recommend it to be run on an AWS Spark cluster for the processing and using AWS S3 and redshift for the staging and final tables respectively. All of these systems are designed to be scalable up to terabytes of data.\n",
    "\n",
    "##### - Daily updates at 7am\n",
    "This is possible. Before 7am the previous day's measurements would be gathered and pushed to a data lake or data warehouse. A dashboard such as Tableau can be connected to the data lake or data warehouse.\n",
    "\n",
    "##### - 100+ people to access the database.\n",
    "To achieve this the tables will need to be hosted in a service like AWS Redshift or via a company owned database system. If there a specific types of queries that are being run, then the tables could be changed or added to incorporate these specific queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
